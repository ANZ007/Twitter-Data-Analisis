{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbe8ce7-0156-4126-bcec-167b08a4854c",
   "metadata": {},
   "source": [
    "# Pelabelan Data Menggunakan Pretrained Model\n",
    "Menggunakan Transformer dengan Model Indonesian RoBERTa Base Sentiment Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aafbb06-0f46-407d-b146-3cf49a6bc948",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f57409-d8d6-4461-8388-fdb4e3712c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-21 12:24:14.654218: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca927c0-f90f-48aa-b5af-44a294aaaffe",
   "metadata": {},
   "source": [
    "## Import Data dari Proses Sebelumnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e9cf2b-8c40-4836-a5e8-d45a634e192e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['menangani', 'kekerasan', 'seksual', 'disahka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['menangani', 'kekerasan', 'seksual', 'disahka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['wakil', 'mpr', 'ri', 'mahasiswa', 'kawal', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['fadel', 'muhammad', 'mahasiswa', 'kawal', 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['tanggal', 'chatnya', 'april', 'dijerat']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  ['menangani', 'kekerasan', 'seksual', 'disahka...\n",
       "1  ['menangani', 'kekerasan', 'seksual', 'disahka...\n",
       "2  ['wakil', 'mpr', 'ri', 'mahasiswa', 'kawal', '...\n",
       "3  ['fadel', 'muhammad', 'mahasiswa', 'kawal', 'i...\n",
       "4         ['tanggal', 'chatnya', 'april', 'dijerat']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/tweets_clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c46481-d868-4737-b9cf-e705eec78652",
   "metadata": {},
   "source": [
    "## Detokenizing\n",
    "### Membuat kata - kata yang telah tertoken menjadi kalimat biasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b5a0ac-3235-44b6-891e-fb731c4ecb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>menangani  kekerasan  seksual  disahkan  enam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>menangani  kekerasan  seksual  disahkan  enam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wakil  mpr  ri  mahasiswa  kawal  implementasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fadel  muhammad  mahasiswa  kawal  implementasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanggal  chatnya  april  dijerat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  menangani  kekerasan  seksual  disahkan  enam ...\n",
       "1  menangani  kekerasan  seksual  disahkan  enam ...\n",
       "2     wakil  mpr  ri  mahasiswa  kawal  implementasi\n",
       "3    fadel  muhammad  mahasiswa  kawal  implementasi\n",
       "4                   tanggal  chatnya  april  dijerat"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_detokenize = []\n",
    "\n",
    "def detokenize(text):\n",
    "    text1 = text.replace(']','').replace('[','')\n",
    "    arr = text1.replace('\"','').replace(\"\\'\",\"\").split(\",\")\n",
    "    return(TreebankWordDetokenizer().detokenize(arr))\n",
    "\n",
    "df['tweet'] = df['tweet'].astype('U').apply(detokenize)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d046d2-89e4-4fc3-a1c6-d55c8261c189",
   "metadata": {},
   "source": [
    "## Mempersiapkan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f61fac-39d7-4981-bb76-789bfa776076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-21 12:24:20.985874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-21 12:24:21.006783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-21 12:24:21.006882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-21 12:24:21.007212: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-21 12:24:21.008196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-21 12:24:21.008296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-21 12:24:21.008366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-21 12:24:21.295284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-21 12:24:21.295420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-21 12:24:21.295497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-21 12:24:21.295573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4129 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-08-21 12:24:21.929416: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at w11wo/indonesian-roberta-base-sentiment-classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "pretrained_name = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
    "\n",
    "nlp = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=pretrained_name,\n",
    "    tokenizer=pretrained_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb50c5-a718-48f7-95bd-236111668feb",
   "metadata": {},
   "source": [
    "## Polarity Scoring and Labelling\n",
    "### Menilai polaritas yang terdapat pada kata dan melabelkannya bedasarkan nilai polaritasnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90fcf103-0735-4b55-92f3-9dc8c2b034d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f779c655cb0454b80970f3d019e87b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring and Labelling..:   0%|          | 0/15632 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"embeddings\" (type TFRobertaEmbeddings).\n\nValue for attr 'Tindices' of float is not in the list of allowed values: int32, int64\n\t; NodeDef: {{node ResourceGather}}; Op<name=ResourceGather; signature=resource:resource, indices:Tindices -> output:dtype; attr=batch_dims:int,default=0; attr=validate_indices:bool,default=true; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true> [Op:ResourceGather]\n\nCall arguments received by layer \"embeddings\" (type TFRobertaEmbeddings):\n  • input_ids=tf.Tensor(shape=(1, 0), dtype=float32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(1, 0), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentimen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskor\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m],desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScoring and Labelling..\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m     score \u001b[38;5;241m=\u001b[39m nlp(i)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m     df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df2, pd\u001b[38;5;241m.\u001b[39mDataFrame([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m\"\u001b[39m : i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentimen\u001b[39m\u001b[38;5;124m\"\u001b[39m : sentiment, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskor\u001b[39m\u001b[38;5;124m\"\u001b[39m : score}])])\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:138\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/pipelines/base.py:1067\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/pipelines/base.py:1074\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1073\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1074\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1075\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/pipelines/base.py:978\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    977\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    980\u001b[0m     inference_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_inference_context()\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:165\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:413\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    412\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:1359\u001b[0m, in \u001b[0;36mTFRobertaForSequenceClassification.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(ROBERTA_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1351\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1352\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;124;03m    labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1359\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1372\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:413\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    412\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:655\u001b[0m, in \u001b[0;36mTFRobertaMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    653\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfill(dims\u001b[38;5;241m=\u001b[39minput_shape, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 655\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m attention_mask_shape \u001b[38;5;241m=\u001b[39m shape_list(attention_mask)\n",
      "File \u001b[0;32m/media/DATA/Apps-Files/Linux/Miniconda3/envs/data-mining/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:149\u001b[0m, in \u001b[0;36mTFRobertaEmbeddings.call\u001b[0;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m shape_list(inputs_embeds)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"embeddings\" (type TFRobertaEmbeddings).\n\nValue for attr 'Tindices' of float is not in the list of allowed values: int32, int64\n\t; NodeDef: {{node ResourceGather}}; Op<name=ResourceGather; signature=resource:resource, indices:Tindices -> output:dtype; attr=batch_dims:int,default=0; attr=validate_indices:bool,default=true; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true> [Op:ResourceGather]\n\nCall arguments received by layer \"embeddings\" (type TFRobertaEmbeddings):\n  • input_ids=tf.Tensor(shape=(1, 0), dtype=float32)\n  • position_ids=None\n  • token_type_ids=tf.Tensor(shape=(1, 0), dtype=int32)\n  • inputs_embeds=None\n  • past_key_values_length=0\n  • training=False"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame(columns = ['tweet', 'sentimen', 'skor'])\n",
    "for i in tqdm(df['tweet'],desc='Scoring and Labelling..'):\n",
    "    sentiment = nlp(i)[0]['label']\n",
    "    score = nlp(i)[0]['score']\n",
    "    df2 = pd.concat([df2, pd.DataFrame([{\"tweet\" : i, \"sentimen\" : sentiment, \"skor\" : score}])])\n",
    "    \n",
    "def change_languange(text):\n",
    "    if text == \"neutral\":\n",
    "        return \"Netral\"\n",
    "    if text == \"positive\":\n",
    "        return \"Positif\"\n",
    "    if text == \"negative\":\n",
    "        return \"Negatif\"\n",
    "\n",
    "df2['sentimen'] = df2['sentimen'].apply(change_languange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee5cdf-6903-4eb2-8c3d-8499e319b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d38a51-d38a-4f88-a889-82ff6116349d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ace1a6-b111-4192-997d-c8d6bb45c940",
   "metadata": {},
   "source": [
    "### Total tweet positif, negatif, atau netral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8426059-5867-493c-84a8-94043cd0432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positif :\",len(df2[df2.sentimen==\"Positif\"]), \" tweet\")\n",
    "print(\"Netral :\",len(df2[df2.sentimen==\"Netral\"]), \" tweet\")\n",
    "print(\"Negatif :\",len(df2[df2.sentimen==\"Negatif\"]), \" tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2d8c3-969c-4547-ba39-04bd4bead7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 30):\n",
    "#      with open('data/daftar_kata.txt', 'w') as f:\n",
    "#         print(df2['tweet'].str.split(expand=True).stack().value_counts(), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efbc35-1fe6-4858-9e0e-846aebde89a5",
   "metadata": {},
   "source": [
    "### Pie chart dari data labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50058c-9d9e-41ae-94dc-8343bc575bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([len(df2[df2.sentimen==\"Positif\"]),  len(df2[df2.sentimen==\"Netral\"]), len(df2[df2.sentimen==\"Negatif\"])])\n",
    "mylabels = ['Positif', 'Netral', 'Negatif']\n",
    "mycolors = ['lightblue', 'lightgreen', 'orange']\n",
    "myexplode = [0, 0.2, 0]\n",
    "\n",
    "plt.rcParams['text.color'] = 'black'\n",
    "plt.pie(y, colors=mycolors, labels = mylabels, explode = myexplode, shadow=True, autopct='%1.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd4cb2-e264-4a4c-9d44-3aa3c94b32e4",
   "metadata": {},
   "source": [
    "### Wordcloud semua data, positif, netral, dan negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a57c98-aa8a-48ea-a3c6-3a00e0d3bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(\"data/cloud.png\"))\n",
    "\n",
    "def plot_cloud(title, text):\n",
    "    wc = WordCloud(scale=3,max_words=100,font_path=\"data/font/GothamMedium.ttf\",background_color='white',\n",
    "                   mask=mask,contour_color='black',contour_width=1).generate(str(\" \".join(text)))\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(40,30))\n",
    "    # Insert image wordcloud\n",
    "    plt.imshow(wc) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\")\n",
    "    # Add Title\n",
    "    plt.title(title)\n",
    "    # Display image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8ee3d-47c1-4167-b63c-6389e4cacdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df2['tweet'].astype('U')\n",
    "\n",
    "plot_cloud(\"Semua Data\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a318d7a-07fb-4022-8d34-63906c6da8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pos = df2[df2.sentimen==\"Positif\"].tweet.astype('U')\n",
    "\n",
    "plot_cloud(\"Data Positif\", text_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f7309-107e-4baa-a2aa-448b1a721036",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_net = df2[df2.sentimen==\"Netral\"].tweet.astype('U')\n",
    "\n",
    "plot_cloud(\"Data Netral\", text_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae616896-9f7a-4372-8ed0-d0335e11302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_neg = df2[df2.sentimen==\"Negatif\"].tweet.astype('U')\n",
    "\n",
    "plot_cloud(\"Data Negatif\", text_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50b5c7-208c-49cd-a9e9-5fcee622d200",
   "metadata": {},
   "source": [
    "## Export data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5488af7-f022-4394-9538-0a424ef0330d",
   "metadata": {},
   "source": [
    "### Ekspor data per kata positif, negatif, atau netral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee768c72-f379-4844-92be-ca46e512cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_positif = df2[df2.sentimen==\"Positif\"]\n",
    "df2_netral = df2[df2.sentimen==\"Netral\"]\n",
    "df2_negatif = df2[df2.sentimen==\"Negatif\"]\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 30):\n",
    "    with open('data/temp/daftar_kata_all_roberta.txt', 'w') as f:\n",
    "        print(df2['tweet'].str.split(expand=True).stack().value_counts(), file=f)\n",
    "    with open('data/temp/daftar_kata_positif_roberta.txt', 'w') as f:\n",
    "        print(df2_positif['tweet'].str.split(expand=True).stack().value_counts(), file=f)\n",
    "    with open('data/temp/daftar_kata_netral_roberta.txt', 'w') as f:\n",
    "        print(df2_netral['tweet'].str.split(expand=True).stack().value_counts(), file=f)\n",
    "    with open('data/temp/daftar_kata_negatif_roberta.txt', 'w') as f:\n",
    "        print(df2_negatif['tweet'].str.split(expand=True).stack().value_counts(), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b295f9-d406-45df-9152-9b16b290b0c9",
   "metadata": {},
   "source": [
    "### Ekspor data labelling untokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7a0a1-499b-4af3-9909-c2581e6a48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"data/tweets_labelled_roberta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad398151-000d-4bc9-94ba-521d9e005b29",
   "metadata": {},
   "source": [
    "### Ekspor data labelling tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564608f-0357-481b-a8a7-67bf7c810ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df2['tweet'] = df2['tweet'].apply(word_tokenize_wrapper)\n",
    "df2.to_csv(\"data/tweets_labelled_tokenized_roberta.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c3e6423842ed7db2f96ef7b0b918d43a5807d8b2148664ac76af26d1a8eb93f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
